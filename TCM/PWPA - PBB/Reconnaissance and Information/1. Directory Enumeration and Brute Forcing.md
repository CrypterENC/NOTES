[[0. Website Used for Practicing PBB]]
### What is **Directory Enumeration ?**
- Directory enumeration and brute forcing **is about discovering hidden files and folders on a web server using wordlists and automated tools**, so you can **find admin panels, backups, and other juicy targets.**
### Core CLI tools (Kali focus)

#### **Gobuster**: Fast Go‑based tool for directories and VHosts.

- Basic: `gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- With extensions: `-x php,txt,html`
- Recursion control: `-r` (recursive), `-r=false` (non‑recursive).
```shell
# Basic directory brute-forcing
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Recursive scanning
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r

# Non-recursive scan (faster)
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r=false
```
#### **Dirsearch**: Python path scanner with many options. 

- Basic: `python3 dirsearch.py -u http://target.com -e php,html,txt`
- Recursive: `-r`
- Through Burp: `--proxy http://127.0.0.1:8080`
 - Save report: `-o report.txt`.
 ```shell
 # Basic scan
python3 dirsearch.py -u http://target.com -e php,html,txt

# Recursive scan
python3 dirsearch.py -u http://target.com -e php,html,txt -r

# With proxy
python3 dirsearch.py -u http://target.com -e php,html,txt --proxy http://127.0.0.1:8080

# Save output to file
python3 dirsearch.py -u http://target.com -e php,html,txt -o report.txt
 ```
#### **FFUF**: Very flexible fuzzer for dirs, vhosts, params.

- Basic: `ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt`
- Extensions: `-e .php,.txt,.html`
- Recursion: `-recursion`
- Filter by size: `-fs 0`. 
```shell
# Basic directory fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt

# With extensions
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -e .php,.txt,.html

# Recursive fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -recursion

# Filter by response size
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 0
```
#### **Dirb**: Classic scanner using built‑in wordlists.

- Basic: `dirb http://target.com /usr/share/wordlists/dirb/common.txt`
- Extensions: `-X .php,.txt`
- No forced extensions: `-N`
- Delay: `-z 100`. 
```shell
# Basic scan
dirb http://target.com /usr/share/wordlists/dirb/common.txt

# With extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt

# Don't force extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt -N

# Set delay between requests
dirb http://target.com /usr/share/wordlists/dirb/common.txt -z 100
```
#### **Wfuzz**: Generic web fuzzer; supports multiple wordlists.

- Basic dirs: `wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ`
- Multiple wordlists and EXT fuzzing for complex patterns. 
```shell
# Basic directory fuzzing
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ

# With extensions
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt -z list,"php-txt-html" -d "file=FUZZ.EXT" http://target.com

# Using multiple wordlists
wfuzz -c -z file,wordlist1.txt -z file,wordlist2.txt http://target.com/FUZZ1/FUZZ2
```
#### **Feroxbuster**: Very fast recursive discovery.

- Basic: `feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- Extensions: `-x php,txt,html`
- Recursion depth: `-d 3`.​ 
```shell
# Basic scan
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Set recursion depth
feroxbuster -u http://target.com -w wordlist.txt -d 3
```
### **GUI / scanner tools with enum features :** 

#### **Dirbuster** (GUI):

-  Start: `dirbuster`
- Features: wordlist support, recursion, filters, result export, easy for visual use.
#### **Arachni**: Full web scanner with common/backup file checks.

- Basic: `arachni http://target.com`
- Specific checks: `--checks=common_files,backup_files`.
```shell
# Basic scan
arachni http://target.com

# With specific checks
arachni --checks=common_files,backup_files http://target.com

# Save report
arachni http://target.com --report-save-path=report.afr
```
#### **Nikto**: Web server scanner that also finds dangerous/interesting files.

- Basic: `nikto -h http://target.com`
- SSL: `-h https://target.com -ssl`.
```shell
# Basic scan
nikto -h http://target.com

# Scan with SSL
nikto -h https://target.com -ssl

# Save output
nikto -h http://target.com -o report.html -Format htm
```
#### **Wapiti**: Scanner with directory discovery.

- Basic: `wapiti -u http://target.com`
- Auth: `--auth-method=basic --auth-user=admin --auth-password=password`.
```shell
# Basic scan
wapiti -u http://target.com

# Scan with authentication
wapiti -u http://target.com --auth-method=basic --auth-user=admin --auth-password=password

# Generate HTML report
wapiti -u http://target.com -f html -o /path/to/report
```

### Wordlists and strategy

#### **Built‑in wordlists (Kali)**:

- `/usr/share/wordlists/dirb/common.txt, big.txt, small.txt`
- `/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt`
- `/usr/share/wordlists/seclists/Discovery/Web-Content/raft-*.txt`.

#### **Recommended Wordlist Resources**

1. **SecLists**  [GitHub](https://github.com/danielmiessler/SecLists)  
   The most comprehensive collection of security wordlists, including directory brute-forcing, file names, and common passwords.

2. **AssetNote Wordlists**  [Website](https://wordlists.assetnote.io/)  
   High-quality, regularly updated wordlists including directory brute-forcing and API endpoints.

3. **FuzzDB**  [GitHub](https://github.com/fuzzdb-project/fuzzdb)  
   Extensive collection of attack patterns and wordlists for various web vulnerabilities.

4. **Common Web Files**  [GitHub](https://github.com/danielmiessler/RobotsDisallowed)  
   Collection of paths found in robots.txt files.

5. **Bug Bounty Wordlist**  [GitHub](https://github.com/assetnote/commonspeak2-wordlists)  
   Generated from real-world data, great for bug bounty hunting.

6. **LFI Wordlist**  [GitHub](https://github.com/danielmiessler/SecLists/blob/master/Fuzzing/LFI/LFI-Jhaddix.txt)  
   Specifically for Local File Inclusion attacks.
### Best Practices

1. **Start Small**
   - Begin with smaller wordlists before moving to larger ones
   - Use common extensions first (.php, .html, .txt, .js, etc.)

2. **Be Stealthy**
   - Use rate limiting to avoid detection
   - Randomize user agents
   - Respect robots.txt (but don't rely on it for security)

3. **Check for Common Files**
   - Configuration files (.env, config.php, .git/config)
   - Backup files (.bak, .old, .backup, .swp)
   - Version control files (.git/, .svn/, .hg/)
   - Common administrative interfaces (/admin, /login, /cpanel)

4. **Analyze Results**
   - Look for interesting response codes (200, 301, 302, 403, 500)
   - Check for interesting file sizes (very small or very large responses)
   - Look for common patterns in responses

5. **Documentation**
   - Save your commands and results
   - Take screenshots of interesting findings
   - Document the date and time of your scans
   
5. **Legal Considerations**
   - Always get proper authorization before scanning
   - Be aware of local laws and regulations
   - Respect rate limits and terms of service


