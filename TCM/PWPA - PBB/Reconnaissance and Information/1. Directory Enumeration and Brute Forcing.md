[[0. Website Used for Practicing PBB]]
### What is **Directory Enumeration ?**
- Directory enumeration and brute forcing **is about discovering hidden files and folders on a web server using wordlists and automated tools**, so you can **find admin panels, backups, and other juicy targets.**
### Core CLI tools (Kali focus)

#### **Gobuster**: Fast Goâ€‘based tool for directories and VHosts.

- Basic:Â `gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- With extensions:Â `-x php,txt,html`
- Recursion control:Â `-r`Â (recursive),Â `-r=false`Â (nonâ€‘recursive).
```shell
# Basic directory brute-forcing
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Recursive scanning
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r

# Non-recursive scan (faster)
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r=false
```
#### **Dirsearch**: Python path scanner with many options. 

- Basic:Â `python3 dirsearch.py -u http://target.com -e php,html,txt`
- Recursive:Â `-r`
- Through Burp:Â `--proxy http://127.0.0.1:8080`
 - Save report:Â `-o report.txt`.
 ```shell
 # Basic scan
python3 dirsearch.py -u http://target.com -e php,html,txt

# Recursive scan
python3 dirsearch.py -u http://target.com -e php,html,txt -r

# With proxy
python3 dirsearch.py -u http://target.com -e php,html,txt --proxy http://127.0.0.1:8080

# Save output to file
python3 dirsearch.py -u http://target.com -e php,html,txt -o report.txt
 ```
#### **FFUF**: Very flexible fuzzer for dirs, vhosts, params.

- Basic:Â `ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt`
- Extensions:Â `-e .php,.txt,.html`
- Recursion:Â `-recursion`
- Filter by size:Â `-fs 0`. 
```shell
# Basic directory fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt

# With extensions
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -e .php,.txt,.html

# Recursive fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -recursion

# Filter by response size
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 0
```
#### **Dirb**: Classic scanner using builtâ€‘in wordlists.

- Basic:Â `dirb http://target.com /usr/share/wordlists/dirb/common.txt`
- Extensions:Â `-X .php,.txt`
- No forced extensions:Â `-N`
- Delay:Â `-z 100`. 
```shell
# Basic scan
dirb http://target.com /usr/share/wordlists/dirb/common.txt

# With extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt

# Don't force extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt -N

# Set delay between requests
dirb http://target.com /usr/share/wordlists/dirb/common.txt -z 100
```
#### **Wfuzz**: Generic web fuzzer; supports multiple wordlists.

- Basic dirs:Â `wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ`
- Multiple wordlists and EXT fuzzing for complex patterns. 
```shell
# Basic directory fuzzing
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ

# With extensions
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt -z list,"php-txt-html" -d "file=FUZZ.EXT" http://target.com

# Using multiple wordlists
wfuzz -c -z file,wordlist1.txt -z file,wordlist2.txt http://target.com/FUZZ1/FUZZ2
```
#### **Feroxbuster**: Very fast recursive discovery.

- Basic:Â `feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- Extensions:Â `-x php,txt,html`
- Recursion depth:Â `-d 3`.â€‹ 
```shell
# Basic scan
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Set recursion depth
feroxbuster -u http://target.com -w wordlist.txt -d 3
```
### **GUI / scanner tools with enum features :** 

#### **Dirbuster**Â (GUI):

-  Start:Â `dirbuster`
- Features: wordlist support, recursion, filters, result export, easy for visual use.
#### **Arachni**: Full web scanner with common/backup file checks.

- Basic:Â `arachni http://target.com`
- Specific checks:Â `--checks=common_files,backup_files`.
```shell
# Basic scan
arachni http://target.com

# With specific checks
arachni --checks=common_files,backup_files http://target.com

# Save report
arachni http://target.com --report-save-path=report.afr
```
#### **Nikto**: Web server scanner that also finds dangerous/interesting files.

- Basic:Â `nikto -h http://target.com`
- SSL:Â `-h https://target.com -ssl`.
```shell
# Basic scan
nikto -h http://target.com

# Scan with SSL
nikto -h https://target.com -ssl

# Save output
nikto -h http://target.com -o report.html -Format htm
```
#### **Wapiti**: Scanner with directory discovery.

- Basic:Â `wapiti -u http://target.com`
- Auth:Â `--auth-method=basic --auth-user=admin --auth-password=password`.
```shell
# Basic scan
wapiti -u http://target.com

# Scan with authentication
wapiti -u http://target.com --auth-method=basic --auth-user=admin --auth-password=password

# Generate HTML report
wapiti -u http://target.com -f html -o /path/to/report
```

### Wordlists and strategy

#### **Builtâ€‘in wordlists (Kali)**:

- `/usr/share/wordlists/dirb/common.txt, big.txt, small.txt`
- `/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt`
- `/usr/share/wordlists/seclists/Discovery/Web-Content/raft-*.txt`.

#### **Recommended Wordlist Resources**

1. **SecLists**  [GitHub](https://github.com/danielmiessler/SecLists)  
   The most comprehensive collection of security wordlists, including directory brute-forcing, file names, and common passwords.

2. **AssetNote Wordlists**  [Website](https://wordlists.assetnote.io/)  
   High-quality, regularly updated wordlists including directory brute-forcing and API endpoints.

3. **FuzzDB**  [GitHub](https://github.com/fuzzdb-project/fuzzdb)  
   Extensive collection of attack patterns and wordlists for various web vulnerabilities.

4. **Common Web Files**  [GitHub](https://github.com/danielmiessler/RobotsDisallowed)  
   Collection of paths found in robots.txt files.

5. **Bug Bounty Wordlist**  [GitHub](https://github.com/assetnote/commonspeak2-wordlists)  
   Generated from real-world data, great for bug bounty hunting.

6. **LFI Wordlist**  [GitHub](https://github.com/danielmiessler/SecLists/blob/master/Fuzzing/LFI/LFI-Jhaddix.txt)  
   Specifically for Local File Inclusion attacks.
### Best Practices

1. **Start Small**
   - Begin with smaller wordlists before moving to larger ones
   - Use common extensions first (.php, .html, .txt, .js, etc.)

2. **Be Stealthy**
   - Use rate limiting to avoid detection
   - Randomize user agents
   - Respect robots.txt (but don't rely on it for security)

3. **Check for Common Files**
   - Configuration files (.env, config.php, .git/config)
   - Backup files (.bak, .old, .backup, .swp)
   - Version control files (.git/, .svn/, .hg/)
   - Common administrative interfaces (/admin, /login, /cpanel)

4. **Analyze Results**
   - Look for interesting response codes (200, 301, 302, 403, 500)
   - Check for interesting file sizes (very small or very large responses)
   - Look for common patterns in responses

5. **Documentation**
   - Save your commands and results
   - Take screenshots of interesting findings
   - Document the date and time of your scans
   
5. **Legal Considerations**
   - Always get proper authorization before scanning
   - Be aware of local laws and regulations
   - Respect rate limits and terms of service

## **ğŸ† Ultimate Directory Enumeration Setup**
#ultimate-directory-enumeration
#### **1. Primary Workhorse : FFUF**

```shell
# Primary
ffuf -u https://TARGET/FUZZ \
  -w /usr/share/wordlists/assetnote/raft-large-words.txt \
  -H "User-Agent: Mozilla/5.0 (BugBountyBot)" \
  -mc 200,204,301,302,307,401,403,405,500 \
  -t 100 \  # Reduce threads for stability
  -timeout 10 \
  -recursion \
  -recursion-depth 3 \
  -e .php,.html,.js,.json,.bak,.swp,.old \
  -of json \
  -o ffuf_scan_full.json
  
# Use small wordlist for speed
ffuf -u https://TARGET/FUZZ \
  -w /usr/share/wordlists/assetnote/raft-small-words.txt \
  -t 150 \
  -mc 200,204,301,302,307,401,403,405,500 \
  timeout 10 \
  -o dirs_small.txt

# Directory-specific scan
ffuf -u https://TARGET/FUZZ/ \
  -w /usr/share/wordlists/assetnote/raft-small-directories.txt \
  -t 100 \
  -mc 200,301,302,403 \
  -o dirs_only.txt
```
####  **2. Deep Recursion Specialist: Feroxbuster**

```shell
# Primary
feroxbuster -u https://target.com \
  -w ~/wordlists/SecLists/Discovery/Web-Content/directory-list-2.3-big.txt \
  -x php,html,js,json,txt,xml,conf,config,bak,backup \
  -C 404,403,429 \
  -t 100 \
  -d 4 \
  --auto-tune \
  --silent \
  -o ferox_results.txt
```
#### **3. Intelligent Crawler: Katana**

```shell
katana -u https://target.com \
  -d 4 \
  -jc \
  -kf \
  -fx \
  -c 50 \
  -f qurl \
  -o katana_crawl.txt
```
### **4. Essential Wordlist Stack**

1. **First:**Â `/assetnote/raft-large-words.txt`Â [AssetNote](https://wordlists.assetnote.io/)
2. **Second:**Â `/SecLists/Discovery/Web-Content/directory-list-2.3-big.txt`
3. **Third:**Â Custom wordlist from previous programs

```bash
#!/bin/bash
echo "[+] Installing AssetNote wordlists..."

WORDLIST_DIR="$HOME/wordlists/assetnote"
mkdir -p $WORDLIST_DIR
cd $WORDLIST_DIR

# Download critical wordlists
echo "[+] Downloading RAFT wordlists..."
wget -q https://wordlists-cdn.assetnote.io/data/manual/raft-small-words.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/raft-medium-words.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/raft-large-words.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/raft-small-directories.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/raft-large-directories.txt

# Download DNS/subdomain wordlists
echo "[+] Downloading DNS wordlists..."
wget -q https://wordlists-cdn.assetnote.io/data/manual/best-dns-wordlist.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/2m-subdomains.txt

# Download tech-specific wordlists
echo "[+] Downloading technology wordlists..."
wget -q https://wordlists-cdn.assetnote.io/data/manual/apache.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/nginx.txt
wget -q https://wordlists-cdn.assetnote.io/data/manual/iis.txt

echo "[+] Wordlists downloaded to: $WORDLIST_DIR"
echo "[+] Total files: $(ls -1 | wc -l)"
echo "[+] Total size: $(du -sh .)"
```

## Using the #ultimate-directory-enumeration on a practical website .

### **1. FFUF on the site**

```shell
ffuf -u https://www.aikido.dev/FUZZ \
  -w /usr/share/wordlists/assetnote/raft-large-words.txt \
  -H "User-Agent: Mozilla/5.0 (BugBountyBot)" \
  -mc 200,204,301,302,307,401,403,405,500 \
  -t 100 \  # Reduce threads for stability
  -timeout 10 \
  -recursion \
  -recursion-depth 3 \
  -e .php,.html,.js,.json,.bak,.swp,.old \
  -of json \
  -o ffuf_scan_full.json
```
#### **1.1 Bypassing Rate Limiting in sites.**

##### **1.1.1 Slow down your requests**
```shell
# Add delays between requests
ffuf -u https://www.aikido.dev/FUZZ \
  -w /usr/share/wordlists/assetnote/raft-large-words.txt \
  -t 20 \  # Reduce threads dramatically
  -p "0.5-2" \  # Random delay between requests
  -rate 10 \  # Maximum requests per second
  -timeout 15 \
  -mc 200,204,301,302,307,401,403,405,500 \
  -c
```
##### **1.1.2. Use Smaller Wordlist First**
```shell
# Test with small wordlist to gauge limits
ffuf -u https://www.aikido.dev/FUZZ \
  -w /usr/share/wordlists/assetnote/raft-small-words.txt \
  -t 30 \
  -p "0.3-1" \
  -rate 15
```

##### **1.1.3. ğŸ›¡ï¸ Stealth Mode Techniques -- Mimic Legitimate Traffic
```shell
# Add realistic headers
ffuf -u https://www.aikido.dev/FUZZ \
  -w wordlist.txt \
  -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" \
  -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8" \
  -H "Accept-Language: en-US,en;q=0.5" \
  -H "Accept-Encoding: gzip, deflate" \
  -H "Connection: keep-alive" \
  -H "Upgrade-Insecure-Requests: 1" \
  -H "Cache-Control: max-age=0" \
  -t 15 \
  -p "1-5" \
  -rate 5
```
#### **1.2 Use JQ** ( ## **JQ (Command-line JSON Processor) - BEST FOR TERMINAL** )
```shell
# Install jq if not present
sudo apt install jq

# Show all URLs with status codes
jq -r '.results[] | "\(.status): \(.url)"' ffuf_scan.json

# Sort by status code
jq -r '.results[] | "\(.status): \(.url)"' ffuf_scan.json | sort -n

# Show only 200/301/302 responses
jq -r '.results[] | select(.status==200 or .status==301 or .status==302) | "\(.status): \(.url)"' ffuf_scan.json

# Show results with length and content-type
jq -r '.results[] | "\(.status) [\(.length)] \(.content-type): \(.url)"' ffuf_scan.json

# Create CSV output
jq -r '.results[] | [.status, .length, .url, .content-type] | @csv' ffuf_scan.json > results.csv
```