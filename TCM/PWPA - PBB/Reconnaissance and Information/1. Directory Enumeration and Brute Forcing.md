[[0. Website Used for Practicing PBB]]
### What is **Directory Enumeration ?**
- Directory enumeration and brute forcing **is about discovering hidden files and folders on a web server using wordlists and automated tools**, so you can **find admin panels, backups, and other juicy targets.**
### Core CLI tools (Kali focus)

#### **Gobuster**: Fast Go‚Äëbased tool for directories and VHosts.

- Basic:¬†`gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- With extensions:¬†`-x php,txt,html`
- Recursion control:¬†`-r`¬†(recursive),¬†`-r=false`¬†(non‚Äërecursive).
```shell
# Basic directory brute-forcing
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Recursive scanning
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r

# Non-recursive scan (faster)
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r=false
```
#### **Dirsearch**: Python path scanner with many options. 

- Basic:¬†`python3 dirsearch.py -u http://target.com -e php,html,txt`
- Recursive:¬†`-r`
- Through Burp:¬†`--proxy http://127.0.0.1:8080`
 - Save report:¬†`-o report.txt`.
 ```shell
 # Basic scan
python3 dirsearch.py -u http://target.com -e php,html,txt

# Recursive scan
python3 dirsearch.py -u http://target.com -e php,html,txt -r

# With proxy
python3 dirsearch.py -u http://target.com -e php,html,txt --proxy http://127.0.0.1:8080

# Save output to file
python3 dirsearch.py -u http://target.com -e php,html,txt -o report.txt
 ```
#### **FFUF**: Very flexible fuzzer for dirs, vhosts, params.

- Basic:¬†`ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt`
- Extensions:¬†`-e .php,.txt,.html`
- Recursion:¬†`-recursion`
- Filter by size:¬†`-fs 0`. 
```shell
# Basic directory fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt

# With extensions
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -e .php,.txt,.html

# Recursive fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -recursion

# Filter by response size
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 0
```
#### **Dirb**: Classic scanner using built‚Äëin wordlists.

- Basic:¬†`dirb http://target.com /usr/share/wordlists/dirb/common.txt`
- Extensions:¬†`-X .php,.txt`
- No forced extensions:¬†`-N`
- Delay:¬†`-z 100`. 
```shell
# Basic scan
dirb http://target.com /usr/share/wordlists/dirb/common.txt

# With extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt

# Don't force extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt -N

# Set delay between requests
dirb http://target.com /usr/share/wordlists/dirb/common.txt -z 100
```
#### **Wfuzz**: Generic web fuzzer; supports multiple wordlists.

- Basic dirs:¬†`wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ`
- Multiple wordlists and EXT fuzzing for complex patterns. 
```shell
# Basic directory fuzzing
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ

# With extensions
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt -z list,"php-txt-html" -d "file=FUZZ.EXT" http://target.com

# Using multiple wordlists
wfuzz -c -z file,wordlist1.txt -z file,wordlist2.txt http://target.com/FUZZ1/FUZZ2
```
#### **Feroxbuster**: Very fast recursive discovery.

- Basic:¬†`feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt`
- Extensions:¬†`-x php,txt,html`
- Recursion depth:¬†`-d 3`.‚Äã 
```shell
# Basic scan
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Set recursion depth
feroxbuster -u http://target.com -w wordlist.txt -d 3
```
### **GUI / scanner tools with enum features :** 

#### **Dirbuster**¬†(GUI):

-  Start:¬†`dirbuster`
- Features: wordlist support, recursion, filters, result export, easy for visual use.
#### **Arachni**: Full web scanner with common/backup file checks.

- Basic:¬†`arachni http://target.com`
- Specific checks:¬†`--checks=common_files,backup_files`.
```shell
# Basic scan
arachni http://target.com

# With specific checks
arachni --checks=common_files,backup_files http://target.com

# Save report
arachni http://target.com --report-save-path=report.afr
```
#### **Nikto**: Web server scanner that also finds dangerous/interesting files.

- Basic:¬†`nikto -h http://target.com`
- SSL:¬†`-h https://target.com -ssl`.
```shell
# Basic scan
nikto -h http://target.com

# Scan with SSL
nikto -h https://target.com -ssl

# Save output
nikto -h http://target.com -o report.html -Format htm
```
#### **Wapiti**: Scanner with directory discovery.

- Basic:¬†`wapiti -u http://target.com`
- Auth:¬†`--auth-method=basic --auth-user=admin --auth-password=password`.
```shell
# Basic scan
wapiti -u http://target.com

# Scan with authentication
wapiti -u http://target.com --auth-method=basic --auth-user=admin --auth-password=password

# Generate HTML report
wapiti -u http://target.com -f html -o /path/to/report
```

### Wordlists and strategy

#### **Built‚Äëin wordlists (Kali)**:

- `/usr/share/wordlists/dirb/common.txt, big.txt, small.txt`
- `/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt`
- `/usr/share/wordlists/seclists/Discovery/Web-Content/raft-*.txt`.

#### **Recommended Wordlist Resources**

1. **SecLists**  [GitHub](https://github.com/danielmiessler/SecLists)  
   The most comprehensive collection of security wordlists, including directory brute-forcing, file names, and common passwords.

2. **AssetNote Wordlists**  [Website](https://wordlists.assetnote.io/)  
   High-quality, regularly updated wordlists including directory brute-forcing and API endpoints.

3. **FuzzDB**  [GitHub](https://github.com/fuzzdb-project/fuzzdb)  
   Extensive collection of attack patterns and wordlists for various web vulnerabilities.

4. **Common Web Files**  [GitHub](https://github.com/danielmiessler/RobotsDisallowed)  
   Collection of paths found in robots.txt files.

5. **Bug Bounty Wordlist**  [GitHub](https://github.com/assetnote/commonspeak2-wordlists)  
   Generated from real-world data, great for bug bounty hunting.

6. **LFI Wordlist**  [GitHub](https://github.com/danielmiessler/SecLists/blob/master/Fuzzing/LFI/LFI-Jhaddix.txt)  
   Specifically for Local File Inclusion attacks.
### Best Practices

1. **Start Small**
   - Begin with smaller wordlists before moving to larger ones
   - Use common extensions first (.php, .html, .txt, .js, etc.)

2. **Be Stealthy**
   - Use rate limiting to avoid detection
   - Randomize user agents
   - Respect robots.txt (but don't rely on it for security)

3. **Check for Common Files**
   - Configuration files (.env, config.php, .git/config)
   - Backup files (.bak, .old, .backup, .swp)
   - Version control files (.git/, .svn/, .hg/)
   - Common administrative interfaces (/admin, /login, /cpanel)

4. **Analyze Results**
   - Look for interesting response codes (200, 301, 302, 403, 500)
   - Check for interesting file sizes (very small or very large responses)
   - Look for common patterns in responses

5. **Documentation**
   - Save your commands and results
   - Take screenshots of interesting findings
   - Document the date and time of your scans
   
5. **Legal Considerations**
   - Always get proper authorization before scanning
   - Be aware of local laws and regulations
   - Respect rate limits and terms of service

## **üèÜ Ultimate Directory Enumeration Setup**
#ultimate-directory-enumeration
#### **1. Primary Workhorse : FFUF  + Katana [Crawl discovered endpoints to find linked content]**

```shell
# Primary
ffuf -u https://TARGET/FUZZ -w /usr/share/wordlists/assetnote/raft-large-words.txt -H "User-Agent: Mozilla/5.0 (BugBountyBot)" -mc 200,204,301,302,307,401,403,405,500 -t 100 -timeout 10 -recursion -recursion-depth 3 -e .php,.html,.js,.json,.bak,.swp,.old -of json -o ffuf_scan_full.json
  
# Use small wordlist for speed
ffuf -u https://TARGET/FUZZ -w /usr/share/wordlists/assetnote/raft-small-words.txt -t 150 -mc 200,204,301,302,307,401,403,405,500 timeout 10 -o dirs_small.txt

# Directory-specific scan
ffuf -u https://TARGET/FUZZ/ -w /usr/share/wordlists/assetnote/raft-small-directories.txt -t 100 -mc 200,301,302,403 -o dirs_only.txt

# Katana: Initial crawl of discovered endpoints
cat dirs_small.txt dirs_only.txt | grep -Eo "https?://[^ ]+" | sort -u | \
katana -silent -d 2 -jc -rl 50 -o katana_initial.txt
```
####  **2. Deep Recursion Specialist: Feroxbuster**

```shell
# Primary
feroxbuster -u https://target.com -w ~/wordlists/SecLists/Discovery/Web-Content/directory-list-2.3-big.txt -x php,html,js,json,txt,xml,conf,config,bak,backup -C 404,403,429 -t 100 -d 4 --auto-tune --silent -o ferox_results.txt

# Full scan with recursion
feroxbuster -u https://TARGET -w /usr/share/wordlists/assetnote/raft-medium-words.txt -x php,js,json,html,asp,aspx -t 100 -d 3 --auto-tune -C 404,429 -o ferox_medium.txt
  
# Backup files discovery
ffuf -u https://TARGET/FUZZ -w /usr/share/wordlists/assetnote/raft-small-words.txt -e .bak,.old,.backup,.swp,.swo,.tar,.gz,.zip -t 80 -mc 200,403 -o backups.txt

# Katana: Deep crawl of all discovered paths
cat ferox_medium.txt backups.txt katana_initial.txt | grep -Eo "https?://[^ ]+" | sort -u | \
katana -silent \
  -d 4 \
  -jc \
  -f all \
  -kf robotstxt,sitemapxml \
  -p 2s \
  -rl 30 \
  -o katana_deep.txt
```
#### **3. Technology-Specific Scans**

```shell
# WordPress (if detected)
ffuf -u https://TARGET/FUZZ \
  -w /usr/share/seclists/Discovery/Web-Content/CMS/wp-plugins.fuzz.txt \
  -t 50 \
  -mc 200,301,302,403 \
  -o wp_plugins.txt

# Laravel
ffuf -u https://TARGET/FUZZ \
  -w <(echo -e "_ignition\nvendor\nstorage\n.env\nconfig\nnova\nhorizon\ntelescope") \
  -t 30 \
  -mc 200,403 \
  -o laravel.txt

# Common config files
ffuf -u https://TARGET/FUZZ \
  -w <(echo -e ".env\nconfig.php\ndatabase.yml\nsettings.py\napplication.properties\nweb.config\n.htaccess\nrobots.txt\nsitemap.xml") \
  -t 20 \
  -mc 200,403 \
  -o configs.txt

# Katana: Tech-specific crawling
cat wp_plugins.txt laravel.txt configs.txt | grep -Eo "https?://[^ ]+" | sort -u | \
katana -silent \
  -d 3 \
  -jc \
  -kf "wp-json,wp-admin,wp-content,wp-includes,api,graphql,swagger" \
  -o katana_tech.txt
```
#### **4. üìä POST-PROCESSING & UNIFICATION PIPELINE**

##### **Phase 1: Combine All Results**

```shell
# Create unified results directory
mkdir -p enum_results
cd enum_results

# Combine ALL text-based results
cat ../*.txt ../ferox_results.txt ../ffuf_scan_full.json 2>/dev/null | \
grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" | \
sort -u > all_raw_urls.txt

echo "[+] Raw URLs collected: $(wc -l all_raw_urls.txt)"
```
##### **Phase 2: Clean and Normalize URLs

```bash
# Remove duplicates and normalize
cat all_raw_urls.txt | \
# Remove ports unless non-standard
sed 's/:80\//\//g; s/:443\//\//g' | \
# Normalize trailing slashes
sed 's|/$||' | \
# Remove query parameters for now
cut -d'?' -f1 | \
# Sort and unique
sort -u > all_urls_normalized.txt

echo "[+] Normalized URLs: $(wc -l all_urls_normalized.txt)"
```
##### **Phase 3: Categorize by Type**

```bash
#!/bin/bash
# Categorization script
echo "[+] Categorizing discovered content..."

# 1. API Endpoints
grep -E -i "(api|rest|graphql|soap|json|xml|/v[0-9]/|/api/v|swagger|openapi)" all_urls_normalized.txt | sort -u > api_endpoints.txt
echo "  ‚Üí API Endpoints: $(wc -l api_endpoints.txt)"

# 2. Admin & Auth
grep -E -i "(admin|dashboard|login|logout|signin|signup|register|auth|oauth|secure|private)" all_urls_normalized.txt | sort -u > admin_auth.txt
echo "  ‚Üí Admin/Auth: $(wc -l admin_auth.txt)"

# 3. Backup & Config Files
grep -E -i "(\.bak$|\.old$|\.backup$|\.swp$|\.swo$|\.tar$|\.gz$|\.zip$|config|\.env|settings|\.htaccess)" all_urls_normalized.txt | sort -u > backup_configs.txt
echo "  ‚Üí Backup/Configs: $(wc -l backup_configs.txt)"

# 4. Sensitive Files
grep -E -i "(password|secret|key|credential|token|private|\.pem$|\.key$|\.crt$|\.pfx$|id_rsa|shadow)" all_urls_normalized.txt | sort -u > sensitive_files.txt
echo "  ‚Üí Sensitive Files: $(wc -l sensitive_files.txt)"

# 5. Dynamic Pages
grep -E "(\.php$|\.asp$|\.aspx$|\.jsp$|\.jsf$|\.do$|\.action$|\.cgi$|\.pl$|\.py$)" all_urls_normalized.txt | sort -u > dynamic_pages.txt
echo "  ‚Üí Dynamic Pages: $(wc -l dynamic_pages.txt)"

# 6. Static Assets
grep -E "(\.js$|\.css$|\.png$|\.jpg$|\.jpeg$|\.gif$|\.svg$|\.ico$|\.woff$|\.ttf$|\.pdf$|\.doc$|\.xls$)" all_urls_normalized.txt | sort -u > static_assets.txt
echo "  ‚Üí Static Assets: $(wc -l static_assets.txt)"

# 7. Directories (end with /)
grep "/$" all_urls_normalized.txt | sort -u > directories.txt
echo "  ‚Üí Directories: $(wc -l directories.txt)"

# 8. Interesting extensions
grep -E "(\.sql$|\.db$|\.log$|\.txt$|\.xml$|\.yml$|\.yaml$|\.ini$)" all_urls_normalized.txt | sort -u > interesting_extensions.txt
echo "  ‚Üí Interesting Extensions: $(wc -l interesting_extensions.txt)"
```
##### **Phase 4: Validate URLs with httpx**

```bash
# Validate all URLs (fast)
cat all_urls_normalized.txt | httpx \
  -silent \
  -status-code \
  -title \
  -tech-detect \
  -content-length \
  -o validated_urls.txt

echo "[+] Validated URLs: $(wc -l validated_urls.txt)"

# Extract live URLs with interesting status codes
grep -E "200|201|202|301|302|307|401|403|405|500" validated_urls.txt > live_interesting.txt

# Create status code breakdown
echo "[+] Status Code Breakdown:"
cat validated_urls.txt | awk '{print $2}' | sort | uniq -c | sort -rn
```
##### **Phase 5: ### Clean and Normalize URLs
##### **Phase 6: ### Clean and Normalize URLs

## Using the #ultimate-directory-enumeration on a practical website .

